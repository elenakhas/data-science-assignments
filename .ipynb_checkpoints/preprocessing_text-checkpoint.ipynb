{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises Lecture 7: Preprocessing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK and SpaCy\n",
    "\n",
    "* Tokenizing, POS tagging, Stemming, Lemmatizing, Parsing, Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install NLTK, please try to run the following cell. If this does not work, please try and follow the [documentation](http://www.nltk.org/install.html). \n",
    "\n",
    "To install SpaCy, look [here](https://spacy.io/usage/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install SpaCy, try the commands below. If this does not work, look [here](https://spacy.io/usage/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conda install -c conda-forge spacy\n",
    "#python -m spacy download en\n",
    "#spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Tokenization and Sentence Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** Tokenizing a text file \n",
    "\n",
    "\n",
    "* Download the data files used for this exercise sheet [here](https://mastertal.gitlab.io/UE803/l7_data.tgz)\n",
    "* Open and read the file 'data/hp.txt'. Then, tokenize it using NLTK word_tokenize module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Harry', 'Potter', 'and', 'the', 'Sorcerer', \"'s\", 'Stone', 'CHAPTER', 'ONE', 'THE', 'BOY', 'WHO', 'LIVED', 'Mr.', 'and', 'Mrs.', 'Dursley', ',', 'of', 'number', 'four', ',', 'Privet', 'Drive', 'in', 'Stansted', ',', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', ',', 'thank', 'you', 'very', 'much', '.', 'They', 'were', 'the', 'last', 'people', 'you', \"'d\", 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', ',', 'because', 'they', 'just', 'did', \"n't\", 'hold', 'with', 'such', 'nonsense', '.', 'Mr.', 'Dursley', 'was', 'the', 'director', 'of', 'a', 'firm', 'called', 'Grunnings', ',', 'which', 'made', 'drills', '.', 'He', 'was', 'a', 'big', ',', 'beefy', 'man', 'with', 'hardly', 'any', 'neck', ',', 'although', 'he', 'did', 'have', 'a', 'very', 'large', 'mustache', '.', 'Mrs.', 'Dursley', 'was', 'thin', 'and', 'blonde', 'and', 'had', 'nearly', 'twice', 'the', 'usual', 'amount', 'of', 'neck', ',', 'which', 'came', 'in', 'very', 'useful', 'as', 'she', 'spent', 'so', 'much', 'of', 'her', 'time', 'craning', 'over', 'garden', 'fences', ',', 'spying', 'on', 'the', 'neighbors', '.', 'The', 'Dursleys', 'had', 'a', 'small', 'son', 'called', 'Dudley', 'and', 'in', 'their', 'opinion', 'there', 'was', 'no', 'finer', 'boy', 'anywhere', '.', 'The', 'Dursleys', 'had', 'everything', 'they', 'wanted', ',', 'but', 'they', 'also', 'had', 'a', 'secret', ',', 'and', 'their', 'greatest', 'fear', 'was', 'that', 'somebody', 'would', 'discover', 'it', '.', 'They', 'did', \"n't\", 'think', 'they', 'could', 'bear', 'it', 'if', 'anyone', 'found', 'out', 'about', 'the', 'Potters', '.', 'Mrs.', 'Potter', 'was', 'Mrs.', 'Dursley', \"'s\", 'sister', ',', 'but', 'they', 'had', \"n't\", 'met', 'for', 'several', 'years', ';', 'in', 'fact', ',', 'Mrs.', 'Dursley', 'pretended', 'she', 'did', \"n't\", 'have', 'a', 'sister', ',', 'because', 'her', 'sister', 'and', 'her', 'good-for-nothing', 'husband', 'were', 'as', 'unDursleyish', 'as', 'it', 'was', 'possible', 'to', 'be', '.', 'The', 'Dursleys', 'shuddered', 'to', 'think', 'what', 'the', 'neighbors', 'would', 'say', 'if', 'the', 'Potters', 'arrived', 'in', 'the', 'street', '.', 'The', 'Dursleys', 'knew', 'that', 'the', 'Potters', 'had', 'a', 'small', 'son', ',', 'too', ',', 'but', 'they', 'had', 'never', 'even', 'seen', 'him', '.', 'This', 'boy', 'was', 'another', 'good', 'reason', 'for', 'keeping', 'the', 'Potters', 'away', ';', 'they', 'did', \"n't\", 'want', 'Dudley', 'mixing', 'with', 'a', 'child', 'like', 'that', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "with open (\"data/hp.txt\") as input:\n",
    "    content = input.read()\n",
    "    tokens = nltk.word_tokenize(content)\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** Breaking the text into Sentences\n",
    "\n",
    "* Now use the sent_tokenize() function, to segment the text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sent_tokenize in module nltk.tokenize:\n",
      "\n",
      "sent_tokenize(text, language='english')\n",
      "    Return a sentence-tokenized copy of *text*,\n",
      "    using NLTK's recommended sentence tokenizer\n",
      "    (currently :class:`.PunktSentenceTokenizer`\n",
      "    for the specified language).\n",
      "    \n",
      "    :param text: text to split into sentences\n",
      "    :param language: the model name in the Punkt corpus\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/hp.txt\") as input:\n",
    "    content = input.read()\n",
    "    sentences = nltk.sent_tokenize(content)\n",
    "    print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** Get all words which end in \"ed\"\n",
    "Use the python function \"endswith\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stansted', 'involved', 'called', 'called', 'wanted', 'pretended', 'shuddered', 'arrived']\n"
     ]
    }
   ],
   "source": [
    "with open (\"data/hp.txt\") as input:\n",
    "    content = input.read()\n",
    "    tokens = nltk.word_tokenize(content)\n",
    "    ed_words = []\n",
    "    for token in tokens:\n",
    "        if token.endswith(\"ed\"):\n",
    "            ed_words.append(token)\n",
    "print(ed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Part-of-speech (POS) tagging\n",
    "\n",
    "[NLTK Book Chapter](https://www.nltk.org/book/ch05.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise 4: ** Use POS tagging to retrieve only those words which end in \"ed\" and are verbs (the output list should no longer contain the noun \"Stansted\"). \n",
    "\n",
    "* To see how `pos_tag()` can be used, use the `help()` function.  \n",
    "* `pos_tag()` takes a tokenized text as input and returns a list of tuples in which the first element corresponds to the token and the second to its pos-tag.\n",
    "* The POS tag set of the Penn Treebank Project, which can be found [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function pos_tag in module nltk.tag:\n",
      "\n",
      "pos_tag(tokens, tagset=None, lang='eng')\n",
      "    Use NLTK's currently recommended part of speech tagger to\n",
      "    tag the given list of tokens.\n",
      "    \n",
      "        >>> from nltk.tag import pos_tag\n",
      "        >>> from nltk.tokenize import word_tokenize\n",
      "        >>> pos_tag(word_tokenize(\"John's big idea isn't all that bad.\"))\n",
      "        [('John', 'NNP'), (\"'s\", 'POS'), ('big', 'JJ'), ('idea', 'NN'), ('is', 'VBZ'),\n",
      "        (\"n't\", 'RB'), ('all', 'PDT'), ('that', 'DT'), ('bad', 'JJ'), ('.', '.')]\n",
      "        >>> pos_tag(word_tokenize(\"John's big idea isn't all that bad.\"), tagset='universal')\n",
      "        [('John', 'NOUN'), (\"'s\", 'PRT'), ('big', 'ADJ'), ('idea', 'NOUN'), ('is', 'VERB'),\n",
      "        (\"n't\", 'ADV'), ('all', 'DET'), ('that', 'DET'), ('bad', 'ADJ'), ('.', '.')]\n",
      "    \n",
      "    NB. Use `pos_tag_sents()` for efficient tagging of more than one sentence.\n",
      "    \n",
      "    :param tokens: Sequence of tokens to be tagged\n",
      "    :type tokens: list(str)\n",
      "    :param tagset: the tagset to be used, e.g. universal, wsj, brown\n",
      "    :type tagset: str\n",
      "    :param lang: the ISO 639 code of the language, e.g. 'eng' for English, 'rus' for Russian\n",
      "    :type lang: str\n",
      "    :return: The tagged tokens\n",
      "    :rtype: list(tuple(str, str))\n",
      "\n",
      "['involved', 'called', 'called', 'wanted', 'pretended', 'shuddered', 'arrived']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Start by reading the documentation:\n",
    "help(nltk.pos_tag)\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "with open (\"data/hp.txt\") as input:\n",
    "    content = input.read()\n",
    "    tokens = nltk.word_tokenize(content)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    verb_tags = [\"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "    \n",
    "    verbs = []\n",
    "    \n",
    "    for token, tag in tagged_tokens:\n",
    "        \n",
    "        if tag in verb_tags and token.endswith('ed'):\n",
    "            verbs.append(token)\n",
    "            \n",
    "    print(verbs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-d8ac7d8622f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Mr. Dursley was the director of a firm called Grunnings, which made drills.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnlp_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'exists'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "nlp = spacy.load('en')\n",
    "sentence = \"Mr. Dursley was the director of a firm called Grunnings, which made drills.\"\n",
    "nlp_sentence = nlp(sentence)\n",
    "spacy_pos_tagged = [(w, w.tag_, w.pos_) for w in nlp_sentence]\n",
    "\n",
    "pd.DataFrame(spacy_pos_tagged,\n",
    "             columns=['Word', 'POS tag', 'Tag type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to collect all verbs that appear in the text. The same verb occurs in different forms however (e.g., 'is, was, were'). To gather together these forms into a single form ('be'), we can use NLTK lemmatizer (nltk.stem.wordnet.WordNetLemmatizer) which given a token and its POS tag will return its lemma i.e., the word form which is usually found in dictionary entries. \n",
    "\n",
    "** Exercise 5: ** Return all verb lemmas contained in the text. \n",
    "\n",
    "Hint: use the WordNetLemmatizer for this using the lemmatize() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('was', 'be'), ('called', 'call'), ('made', 'make')]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "with open (\"data/hp.txt\") as input:\n",
    "    content = input.read()\n",
    "sentence = \"Mr. Dursley was the director of a firm called Grunnings, which made drills.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "verb_tags = [\"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "verbs = []\n",
    "for token, tag in tagged_tokens:\n",
    "    if tag in verb_tags:\n",
    "        verbs.append(token)\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "verb_lemmas = []\n",
    "for word_form in verbs:\n",
    "    lemma = lemmatizer.lemmatize(word_form, \"v\") \n",
    "    verb_lemmas.append((word_form,lemma))\n",
    "print(verb_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise 6: ** The WordNet Lemmatizer needs to know the POS tag of the word form to be lemmatized. \n",
    "\n",
    "Write a program that for each word in the input text:\n",
    "* gets its POS tag\n",
    "* converts is to the corresponding WordNet POS tag (wn.NOUN, wn.VERB, wn.ADV or wn.ADJ)\n",
    "* lemmatize each word form \n",
    "* output the resulting list of lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def penn_to_wn(penn_tag):\n",
    "    if penn_tag in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]:\n",
    "        wn_tag = wn.NOUN\n",
    "    elif penn_tag in ['VB', \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]:\n",
    "        wn_tag = wn.VERB\n",
    "    elif penn_tag in [\"RB\", \"RBR\", \"RBS\"]:\n",
    "        wn_tag = wn.ADV\n",
    "    elif penn_tag in [\"JJ\", \"JJR\", \"JJS\"]:\n",
    "        wn_tag = wn.ADJ\n",
    "    else:\n",
    "        wn_tag = None\n",
    "    return wn_tag\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "lemmas = []\n",
    "\n",
    "for token, pos in tagged_tokens:\n",
    "    wn_tag = penn_to_wn(pos)\n",
    "    if not wn_tag == None:\n",
    "        lemma = lemmatizer.lemmatize(token, wn_tag)\n",
    "    else:\n",
    "        lemma = lemmatizer.lemmatize(token)\n",
    "    lemmas.append(lemma)\n",
    "    print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Processing Multiple Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise 7: ** For each file in \"data/\", print out the number of sentences and the number of words per sentences.\n",
    "\n",
    "* Use Python [glob](https://docs.python.org/3/library/glob.html) module to iterate over all files in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data\\Harry Potter 1 - Sorcerer's Stone.txt has 6396 sentences\n",
      "\n",
      "File data\\Harry Potter 2 - Chamber of Secrets.txt has 6938 sentences\n",
      "\n",
      "File data\\Harry Potter 3 - The Prisoner Of Azkaban.txt has 8585 sentences\n",
      "\n",
      "File data\\hp.txt has 12 sentences\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "for filename in glob.glob(\"data/*.txt\"): \n",
    "    with open(filename, \"r\", encoding=\"utf8\", errors='ignore') as infile:\n",
    "        content = infile.read()\n",
    "    sentences = nltk.sent_tokenize(content)\n",
    "    print(\"File %s has %d sentences\"%(filename, len(sentences)))\n",
    "    counter = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        counter += 1\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        # print(\"Sentence %d has %d tokens\"%(counter, len(tokens)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise 8: ** Extract all noun and verb lemma from the data/ files\n",
    "\n",
    "* create a list of all the files we want to process\n",
    "* open and read the files\n",
    "* tokenize the texts\n",
    "* perform pos-tagging\n",
    "* collect all the tokens analyzed as nouns\n",
    "* print out the number of distinct nouns found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threre are 6995 nouns\n"
     ]
    }
   ],
   "source": [
    "def tag_tokens_file(filepath):\n",
    "    with open(filepath, 'r', encoding = 'utf8', errors='ignore') as input:\n",
    "        content = input.read()\n",
    "        tokens = nltk.word_tokenize(content)\n",
    "        tagged_tokens = nltk.pos_tag(tokens)\n",
    "    return tagged_tokens\n",
    "\n",
    "nouns = []\n",
    "for filename in glob.glob(\"data/*.txt\"):\n",
    "    tagged_tokens = tag_tokens_file(filename)\n",
    "    \n",
    "    for token, pos in tagged_tokens:\n",
    "        if pos in [\"NN\", \"NNP\"]:\n",
    "            nouns.append(token)\n",
    "nouns = set(nouns)\n",
    "print(\"Threre are %d nouns\"%(len(nouns)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parsing \n",
    "\n",
    "** Exercise 9: ** Extract all Noun Phrases from the first 10 sentences of each txt file in the data/ directory.\n",
    "\n",
    "* Open each file\n",
    "* Apply sentence segmentation\n",
    "* Apply the stanford constituency parser to the first 10 sentences of each file\n",
    "* Use the NLTK parse tree methods (help(nltk.tree.Tree)) to retrieve all NPs subtrees and extract their leaves\n",
    "* N.B. The Stanford parser can assign several parses to the same sentence. Only consider the first parse.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "java_path = r'/usr/lib/jvm/java-8-oracle/jre/bin/java'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "scp = StanfordParser(path_to_jar='/home/Lenovo/stanford-parser-full-2018-10-17/stanford-parser.jar',\n",
    "           path_to_models_jar='/home/Lenovo/stanford-parser-full-2018-10-17/stanford-parser-3.9.2-models.jar')\n",
    "\n",
    "for filename in glob.glob(\"data/*.txt\"): \n",
    "    with open(filename, \"r\", encoding=\"utf8\", errors='ignore') as infile:\n",
    "        content = infile.read()\n",
    "    sentences = nltk.sent_tokenize(content)\n",
    "    counter = 0\n",
    "    \n",
    "    for sentence in sentences[0:10]:\n",
    "        counter +=1\n",
    "        parse_trees = list(scp.raw_parse(sentence))\n",
    "        tree = parse_trees[0]\n",
    "        \n",
    "        for s in tree.subtrees(lambda tree: tree.label() == \"NP\"):\n",
    "        print(s.leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. NER \n",
    "\n",
    "** Exercise 10: ** Extract all Person names from the data/ files\n",
    "\n",
    "* Use either Spacy or Stanford NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "import os\n",
    "import pandas as pd\n",
    "java_path = r'/usr/lib/jvm/java-8-oracle/jre/bin/java'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "sner = StanfordNERTagger('/home/Lenovo/stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "           path_to_jar='/home/Lenovo/stanford-ner-2018-10-16/stanford-ner.jar')\n",
    "\n",
    "ne = []\n",
    "\n",
    "for filename in glob.glob(\"data/*.txt\"): \n",
    "    with open(filename, \"r\", encoding=\"utf8\", errors='ignore') as infile:\n",
    "        content = infile.read()\n",
    "    sentences = nltk.sent_tokenize(content)\n",
    "    counter = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        counter +=1\n",
    "        ner_tagged_sent = sner.tag(sentence.split())\n",
    "        sentence_ne = [ne for ne in ner_tagged_sent if ne[1] != '0']\n",
    "        ne = ne + sentence_ne\n",
    "print(set(ne))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
