{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise sheet \\#3\n",
    "\n",
    "Here, you will have to implement a web scraping engine, which will retrieve data from the famous [Internet Movie DataBase](https://www.imdb.com/) (IMDB).\n",
    "The data we are interested in is **movie ratings**. \n",
    "\n",
    "Before starting, we need to keep in mind that requesting webpage could be time consuming. Should we want to build a large dataset, we would need to pay attention to the number of requests sent by our scraping engine (and to their frequency to avoid being black listed by IMDB's servers).\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Let us inspect the following IMDB entry : [Movie tt0211915](https://www.imdb.com/title/tt0211915/).\n",
    "\n",
    "On this page, you can find at least two ratings: the one by IMDB and the one by mediacritic. By pressing Ctrl+U, you can access the page's source code, and on chrome by clicking F12 you can launch the DevTools panel.\n",
    "\n",
    "Where are located these ratings in the underlying HTML code ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_To be completed_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Write a python program which retrieves the page located at the URL given above, parses it and extract the ratings of that movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "69\n",
      " \n",
      "8.3/10 \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "page=requests.get('https://www.imdb.com/title/tt0211915/')\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "# print(soup.prettify())\n",
    "metacritic = soup.find_all(class_='metacriticScore score_favorable titleReviewBarSubItem')[0]\n",
    "score = metacritic.get_text()\n",
    "\n",
    "reviewer = soup.find_all(class_='ratingValue')[0]\n",
    "score2 = reviewer.get_text()\n",
    "print(score, score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "By using IMDB's advanced search feature, can you list the movies released in 2018 by decreasing number of ratings (not descreasing ratings!) ? Describes the actions you performed on IMDB's web interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_To be completed_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The URL of the first page of the search results looks like the following (250 results per page to save time during future retrieval):\n",
    "\n",
    "`https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250`\n",
    "\n",
    "Note also the URL of the second page of the search results:\n",
    "\n",
    "`https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250&start=251&ref_=adv_nxt`\n",
    "\n",
    "### Question 4\n",
    "\n",
    "Write a python program which retrieves the ten first pages of results and store them locally (that is, save a local copy of each of them). Apply a 3-second sleep between requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy, time\n",
    "\n",
    "class movieSpider1(scrapy.Spider):\n",
    "    name = 'MovieSpider1'\n",
    "    start_urls = ['https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250']\n",
    "       \n",
    "    def __init__(self):\n",
    "        super(movieSpider1, self).__init__()\n",
    "        self.counter = 0\n",
    "    \n",
    "    def parse(self, response):\n",
    "        page = response.url.split('/')\n",
    "        filename = 'movie_search-%s.html' % self.counter\n",
    "        self.counter += 1\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "        next_page = response.css('a[class=\"lister-page-next\\ next-page\"]::attr(\"href\")').extract_first()\n",
    "        if next_page is not None and self.counter <= 10:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            time.sleep(3)\n",
    "            #yield response.follow(next_page, callback=self.parse)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Describes the path where the movie information appears in the HTML tree of the retrieved webpages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_To be completed_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "Extend the python program from Question 4 so that the following information is extracted for each movie:\n",
    "\n",
    "- name of the movie\n",
    "- year of release\n",
    "- number of votes\n",
    "- IMDB rating\n",
    "- Metacritic score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class movieSpider2(scrapy.Spider):\n",
    "    name = 'MovieSpider2'\n",
    "    start_urls = ['https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250']\n",
    "       \n",
    "    def __init__(self):\n",
    "        super(movieSpider2, self).__init__()\n",
    "        self.counter = 0\n",
    "    \n",
    "    def parse(self, response):\n",
    "        title = response.css('img[class=\"loadlate\"]::attr(alt)').extract_first()\n",
    "        yest = response.css('span[class=\"lister-item-year text-muted unbold\"]::text').extract_first()\n",
    "        num_votes = response.css('span[name=\"nv\"]::attr(data-value)').extract_first()\n",
    "        rating = response.css('div[class=\"inline-block ratings-imdb-rating\"]::attr(data-value)').extract_first()\n",
    "        meta = response.css('span[class=\"metascore favorable\"]::text').extract_first()\n",
    "        print(title, year, num_votes, rating, meta)\n",
    "        next_page = response.css('a[class=\"lister-page-next next-page\"]::attr(href)').extract_first()\n",
    "        if next_page is not None and self.counter <= 10:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "Extend the python program from Question 6 so that the results of extraction are stored in a [CSV file](https://en.wikipedia.org/wiki/Comma-separated_values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy, csv\n",
    "\n",
    "class movieSpider3(scrapy.Spider):\n",
    "    name = 'MovieSpider3'\n",
    "    start_urls = ['https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250']\n",
    "       \n",
    "    def __init__(self):\n",
    "        super(movieSpider3, self).__init__()\n",
    "        self.counter = 0\n",
    "    \n",
    "    def parse(self, response):\n",
    "        title = response.css('img[class=\"loadlate\"]::attr(alt)').extract_first()\n",
    "        yest = response.css('span[class=\"lister-item-year text-muted unbold\"]::text').extract_first()\n",
    "        num_votes = response.css('span[name=\"nv\"]::attr(data-value)').extract_first()\n",
    "        rating = response.css('div[class=\"inline-block ratings-imdb-rating\"]::attr(data-value)').extract_first()\n",
    "        meta = response.css('span[class=\"metascore favorable\"]::text').extract_first()\n",
    "        with open ('movie_ratings.csv', 'w', newline = '') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            write.writerow(('title', 'year', 'num votes', 'IMDB rating', 'Metacritic score'))\n",
    "            write.writerow((title, year, num_votes, rating, meta))\n",
    "            \n",
    "        next_page = response.css('a[class=\"lister-page-next next-page\"]::attr(href)').extract_first()\n",
    "        if next_page is not None and self.counter <= 10:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "Manually inspect the results of your data extraction. Are there any errors (missing information, ill-formed data, etc.) ? If so, extend your python script to fix these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-14 00:48:41 [scrapy.utils.log] INFO: Scrapy 1.5.2 started (bot: scrapybot)\n",
      "2019-03-14 00:48:41 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1a  20 Nov 2018), cryptography 2.4.2, Platform Windows-10-10.0.17134-SP0\n",
      "2019-03-14 00:48:41 [scrapy.crawler] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2019-03-14 00:48:41 [scrapy.extensions.telnet] INFO: Telnet Password: 6902157c311110ad\n",
      "2019-03-14 00:48:41 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-03-14 00:48:41 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-03-14 00:48:41 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-03-14 00:48:41 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-03-14 00:48:41 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-03-14 00:48:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-03-14 00:48:41 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2019-03-14 00:48:41 [scrapy.crawler] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2019-03-14 00:48:41 [scrapy.extensions.telnet] INFO: Telnet Password: fdc38d7f20bf1a41\n",
      "2019-03-14 00:48:41 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-03-14 00:48:41 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-03-14 00:48:41 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-03-14 00:48:41 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-03-14 00:48:41 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-03-14 00:48:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-03-14 00:48:41 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6024\n",
      "2019-03-14 00:48:41 [scrapy.crawler] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2019-03-14 00:48:41 [scrapy.extensions.telnet] INFO: Telnet Password: a543313d4170da6b\n",
      "2019-03-14 00:48:41 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-03-14 00:48:41 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-03-14 00:48:41 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-03-14 00:48:41 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-03-14 00:48:41 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-03-14 00:48:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-03-14 00:48:41 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6025\n",
      "2019-03-14 00:48:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250> (referer: None)\n",
      "2019-03-14 00:48:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 102, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 30, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 339, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"<ipython-input-12-22585f811e80>\", line 17, in parse\n",
      "    print(title, year, num_votes, rating, meta)\n",
      "NameError: name 'year' is not defined\n",
      "2019-03-14 00:48:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250> (referer: None)\n",
      "2019-03-14 00:48:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250> (referer: None)\n",
      "2019-03-14 00:48:43 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-03-14 00:48:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 304,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 153582,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 3, 13, 23, 48, 43, 134424),\n",
      " 'log_count/DEBUG': 5,\n",
      " 'log_count/ERROR': 1,\n",
      " 'log_count/INFO': 16,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'spider_exceptions/NameError': 1,\n",
      " 'start_time': datetime.datetime(2019, 3, 13, 23, 48, 41, 665877)}\n",
      "2019-03-14 00:48:43 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2019-03-14 00:48:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 102, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 30, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 339, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"<ipython-input-13-6d2fb546388c>\", line 19, in parse\n",
      "    write.writerow(('title', 'year', 'num votes', 'IMDB rating', 'Metacritic score'))\n",
      "NameError: name 'write' is not defined\n",
      "2019-03-14 00:48:46 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-03-14 00:48:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 304,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 153297,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 3, 13, 23, 48, 46, 682087),\n",
      " 'log_count/DEBUG': 4,\n",
      " 'log_count/ERROR': 2,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'spider_exceptions/NameError': 1,\n",
      " 'start_time': datetime.datetime(2019, 3, 13, 23, 48, 41, 681504)}\n",
      "2019-03-14 00:48:46 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2019-03-14 00:48:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250&start=251&ref_=adv_nxt> (referer: https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250)\n",
      "2019-03-14 00:48:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250&start=501&ref_=adv_nxt> (referer: https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250&start=251&ref_=adv_nxt)\n",
      "2019-03-14 00:49:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250&start=751&ref_=adv_nxt> (referer: https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250&start=501&ref_=adv_nxt)\n",
      "2019-03-14 00:49:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250&start=1001&ref_=adv_nxt> (referer: https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250&start=751&ref_=adv_nxt)\n",
      "2019-03-14 00:49:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250&start=1251&ref_=adv_nxt> (referer: https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250&start=1001&ref_=adv_nxt)\n",
      "2019-03-14 00:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250&start=1501&ref_=adv_nxt> (referer: https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250&start=1251&ref_=adv_nxt)\n",
      "2019-03-14 00:49:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250&start=1751&ref_=adv_nxt> (referer: https://www.imdb.com/search/title?release_date=2018-01-01,2018-12-31&sort=num_votes,desc&count=250&start=1501&ref_=adv_nxt)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import scrapy.crawler\n",
    "    myspider = movieSpider1()\n",
    "    myspider2 = movieSpider2()\n",
    "    myspider3 = movieSpider3()\n",
    "    process = scrapy.crawler.CrawlerProcess({'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'})\n",
    "    process.crawl(myspider)\n",
    "    process.crawl(myspider2)\n",
    "    process.crawl(myspider3)\n",
    "    process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "On-line [tutorial](https://www.dataquest.io/blog/web-scraping-beautifulsoup/) entitled \"Web Scraping with Python and BeautifulSoup\" by Alex Olteanu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
